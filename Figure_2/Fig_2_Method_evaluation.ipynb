{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3068be33-489c-440b-9e67-ef43f6c44e90",
   "metadata": {},
   "source": [
    "# Preparing notebook and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61332add-7f87-4b20-bb21-56d6daabb14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH'] = 'true'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "os.environ['PYTHONHASHSEED'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c9341b5-cf33-427e-b498-2cfe5b2ccd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/site-packages/h5py/__init__.py:36: UserWarning: h5py is running against HDF5 1.14.1 when it was built against 1.14.0, this may cause problems\n",
      "  _warn((\"h5py is running against HDF5 {0} when it was built against {1}, \"\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(18)\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "\n",
    "import concurrent.futures\n",
    "import functools\n",
    "\n",
    "from F2_func_file import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3d324a7-fe43-460f-a9ba-32564ec8bd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create folder for storing generated data files\n",
    "if not os.path.exists(\"Data_files\"):\n",
    "    warnings.warn(\"\"\"The folder \"Data_files\" should already exist. Please, run the \"Fig_2_Data_generation\" notebook first before proceeding with the \"Data_evaluation\" one.\"\"\")\n",
    "if not os.path.exists(\"Data_files/Evaluated_data\"):\n",
    "    os.mkdir(\"Data_files/Evaluated_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "579a8cb1-d9e2-40a4-97ad-4d2f9c7acc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the DAMN model\n",
    "DAMN_model = load_model(\"DAMN_model.h5\", custom_objects={\"custom_mse_func\": custom_mse_func, \"custom_mae_func\": custom_mae_func})\n",
    "\n",
    "#Specify the PSF_width value used to generate data for panels (A), (C), and (D); requirement for Richardson-Lucy algorithm\n",
    "PSF_width_value = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b44e9d17-decc-4ddc-803a-983b052341dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To fasten the Richardson_lucy algorithm, we split the data evaluation among several CPU units using ProcessPoolExecutor from concurrent.futures\n",
    "CPU_units_to_use = 10   #Set to 1 if you are not familiar with ProcessPoolExecutor or your CPU availability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c07f754-78ae-4f30-afa7-82713d74ce7d",
   "metadata": {},
   "source": [
    "## Evaluate dataset of the SNR graph - panel (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba04d15b-d6b9-42aa-8077-1663864a56a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shapes: (51, 100, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "#Loading the data generated in the Data_generation notebook\n",
    "data_in_SNR = np.load(\"Data_files/Generated_data/SNR_data_low_res.npy\")\n",
    "data_target_SNR = np.load(\"Data_files/Generated_data/SNR_data_high_res.npy\")\n",
    "\n",
    "shape_SNR = data_in_SNR.shape\n",
    "print(\"Data shapes:\", shape_SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e8cc6f2-6fb9-4b4a-b6a8-b45d18c8c416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data reshaped: (5100, 50, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "#Adjust the data shapes for easier model evaluation\n",
    "data_in_SNR_reshaped = np.reshape(data_in_SNR, (shape_SNR[0]*shape_SNR[1], shape_SNR[2], shape_SNR[3], 1))\n",
    "data_target_SNR_reshaped = np.reshape(data_target_SNR, (shape_SNR[0]*shape_SNR[1], shape_SNR[2], shape_SNR[3], 1))\n",
    "\n",
    "print(\"Data reshaped:\", data_in_SNR_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf35b3b-61f1-48a7-a734-81ea2cb669bd",
   "metadata": {},
   "source": [
    "### DAMN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac380f17-96ff-470f-8c9b-68c61a7f501d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160/160 [==============================] - 15s 82ms/step\n"
     ]
    }
   ],
   "source": [
    "#Use the DAMN model to predict high-resolution images\n",
    "DAMN_model_output_SNR_reshaped = np.squeeze(DAMN_model.predict(data_in_SNR_reshaped))\n",
    "\n",
    "#And return to the original shape\n",
    "DAMN_model_output_SNR = DAMN_model_output_SNR_reshaped.reshape((shape_SNR[0], shape_SNR[1], shape_SNR[2], shape_SNR[3]))\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/SNR_DAMN_model_output.npy\", DAMN_model_output_SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b00d8284-21a7-4348-99d3-2bf5764fd264",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a chosen metric comparing the DAMN output with the corresponding target image\n",
    "DAMN_model_errors_SNR = Evaluate_metric(DAMN_model_output_SNR, data_target_SNR)\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/SNR_DAMN_model_errors.npy\", DAMN_model_errors_SNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34bb6a1-3bf0-450f-9925-00ff3c828911",
   "metadata": {},
   "source": [
    "### Richardson-Lucy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3613c03-cdda-4e57-a641-ba30428b2d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the following data shape for RL slgorithm: (51, 100, 50, 50)\n"
     ]
    }
   ],
   "source": [
    "#As the RL algorithm is very time consuming (matter of up to hours), you can choose to evaluate only a small portion of the data for the graph visualization\n",
    "#Namely, 1/10 of data samples in every 4-th point on the horizontal axis\n",
    "use_all_data_SNR = True             #Switch to False for evaluating only the reduced dataset \n",
    "\n",
    "if use_all_data_SNR:\n",
    "    RL_data_in_SNR = data_in_SNR\n",
    "    RL_data_target_SNR = data_target_SNR\n",
    "    RL_output_SNR = np.zeros(data_in_SNR.shape)\n",
    "else:\n",
    "    RL_data_in_SNR = data_in_SNR[::4,:10]\n",
    "    RL_data_target_SNR = data_target_SNR[::4,:10]\n",
    "    RL_output_SNR = np.zeros(data_in_SNR[::4,:10].shape)\n",
    "\n",
    "print(\"Using the following data shape for RL algorithm:\", RL_data_in_SNR.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10e7f23-55df-40ee-8e16-71ca3ef1f3bb",
   "metadata": {},
   "source": [
    "##### The following cell runs the Richardson-Lucy algorithm, which might turn very time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4b985f-a570-4f19-951e-a8377332aeff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished iteration 1 out of 51 in 116.32 seconds (116.33 from start).\n",
      "Finished iteration 2 out of 51 in 117.26 seconds (233.59 from start).\n",
      "Finished iteration 3 out of 51 in 117.78 seconds (351.37 from start).\n",
      "Finished iteration 4 out of 51 in 120.19 seconds (471.55 from start).\n",
      "Finished iteration 5 out of 51 in 119.48 seconds (591.04 from start).\n",
      "Finished iteration 6 out of 51 in 121.37 seconds (712.41 from start).\n",
      "Finished iteration 7 out of 51 in 122.44 seconds (834.85 from start).\n",
      "Finished iteration 8 out of 51 in 124.21 seconds (959.06 from start).\n",
      "Finished iteration 9 out of 51 in 129.5 seconds (1088.56 from start).\n",
      "Finished iteration 10 out of 51 in 130.58 seconds (1219.14 from start).\n",
      "Finished iteration 11 out of 51 in 132.46 seconds (1351.6 from start).\n",
      "Finished iteration 12 out of 51 in 136.97 seconds (1488.57 from start).\n",
      "Finished iteration 13 out of 51 in 141.07 seconds (1629.64 from start).\n",
      "Finished iteration 14 out of 51 in 144.62 seconds (1774.26 from start).\n",
      "Finished iteration 15 out of 51 in 143.36 seconds (1917.62 from start).\n",
      "Finished iteration 16 out of 51 in 152.48 seconds (2070.11 from start).\n",
      "Finished iteration 17 out of 51 in 152.14 seconds (2222.25 from start).\n"
     ]
    }
   ],
   "source": [
    "#The kernel stays the same for all SNR data samples\n",
    "kernel = Gauss_kernel(PSF_width_value)\n",
    "\n",
    "#The ProcessPoolExecutor calling RL_iteration_for_concurrent function to evaluate the RL_data_in_SNR data\n",
    "start = time.time()\n",
    "for i in range(RL_data_in_SNR.shape[0]):\n",
    "    start_i = time.time()\n",
    "    with concurrent.futures.ProcessPoolExecutor(CPU_units_to_use) as pool:\n",
    "        intermediate_func = functools.partial(RL_iteration_for_concurrent, kernel)\n",
    "        res = pool.map(intermediate_func, RL_data_in_SNR[i])\n",
    "    RL_output_SNR[i] = np.array(list(res))\n",
    "    print(\"Finished iteration\", i+1, \"out of\", RL_data_in_SNR.shape[0], \"in\", np.round(time.time()-start_i, 2), \"seconds (\" + str(np.round(time.time()-start, 2)), \"from start).\")\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/SNR_RL_output.npy\", RL_output_SNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d8e819-f2fc-44ad-8fb4-dee09684eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a chosen metric comparing the DAMN output with the corresponding target image\n",
    "RL_errors_SNR = Evaluate_metric(RL_output_SNR, RL_data_target_SNR)\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/SNR_RL_errors.npy\", RL_errors_SNR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2332153d-d64f-49b7-8854-96b52c88ae2d",
   "metadata": {},
   "source": [
    "## Evaluate dataset of the PSF width graph - panel (B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1f130d-8244-42f0-93d5-310da5382c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data generated in the Data_generation notebook\n",
    "data_in_PSF = np.load(\"Data_files/Generated_data/PSF_data_low_res.npy\")\n",
    "data_target_PSF = np.load(\"Data_files/Generated_data/PSF_data_high_res.npy\")\n",
    "horizontal_axis_PSF = np.load(\"Data_files/Generated_data/PSF_axis_array.npy\")\n",
    "\n",
    "shape_PSF = data_in_PSF.shape\n",
    "print(\"Data shapes:\", shape_PSF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874dfe49-dc8b-41f5-94f3-a8965079e788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust the data shapes for easier model evaluation\n",
    "data_in_PSF_reshaped = np.reshape(data_in_PSF, (shape_PSF[0]*shape_PSF[1], shape_PSF[2], shape_PSF[3], 1))\n",
    "data_target_PSF_reshaped = np.reshape(data_target_PSF, (shape_PSF[0]*shape_PSF[1], shape_PSF[2], shape_PSF[3], 1))\n",
    "\n",
    "print(\"Data reshaped:\", data_in_PSF_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e90d229-beec-432f-8d01-b868eb7c8dbc",
   "metadata": {},
   "source": [
    "### DAMN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b3dda-0e0e-42d1-92c8-bff60c747ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the DAMN model to predict high-resolution images\n",
    "DAMN_model_output_PSF_reshaped = np.squeeze(DAMN_model.predict(data_in_PSF_reshaped))\n",
    "\n",
    "#And return to the original shape\n",
    "DAMN_model_output_PSF = DAMN_model_output_PSF_reshaped.reshape((shape_PSF[0], shape_PSF[1], shape_PSF[2], shape_PSF[3]))\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/PSF_DAMN_model_output.npy\", DAMN_model_output_PSF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b5d5f6-ed82-4a8b-bc85-8163a7022156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a chosen metric comparing the DAMN output with the corresponding target image\n",
    "DAMN_model_errors_PSF = Evaluate_metric(DAMN_model_output_PSF, data_target_PSF)\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/PSF_DAMN_model_errors.npy\", DAMN_model_errors_PSF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029aa198-ba2b-4ace-8770-da7addbe5022",
   "metadata": {},
   "source": [
    "### Richardson-Lucy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d963e8-20ae-4a1d-b7d4-e6fa1d63cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the RL algorithm is very time consuming (matter of up to hours), you can choose to evaluate only a small portion of the data for the graph visualization\n",
    "#Namely, 1/10 of data samples in every 4-th point on the horizontal axis\n",
    "use_all_data_PSF = True             #Switch to False for evaluating only the reduced dataset \n",
    "\n",
    "if use_all_data_PSF:\n",
    "    RL_data_in_PSF = data_in_PSF\n",
    "    RL_data_target_PSF = data_target_PSF\n",
    "    RL_horizontal_axis_PSF = horizontal_axis_PSF\n",
    "    RL_output_PSF = np.zeros(data_in_PSF.shape)\n",
    "else:\n",
    "    RL_data_in_PSF = data_in_PSF[::4,:10]\n",
    "    RL_data_target_PSF = data_target_PSF[::4,:10]\n",
    "    RL_horizontal_axis_PSF = horizontal_axis_PSF[::4]\n",
    "    RL_output_PSF = np.zeros(data_in_PSF[::4,:10].shape)\n",
    "\n",
    "print(\"Using the following data shape for RL slgorithm:\", RL_data_in_PSF.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facf200c-7288-4a3f-a338-29fde797c56b",
   "metadata": {},
   "source": [
    "##### The following cell runs the Richardson-Lucy algorithm, which might turn very time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4c154-9638-4518-87dd-13559b16e0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The ProcessPoolExecutor calling RL_iteration_for_concurrent function to evaluate the RL_data_in_PSF data\n",
    "start = time.time()\n",
    "for i in range(RL_data_in_PSF.shape[0]):\n",
    "    start_i = time.time()\n",
    "\n",
    "    #The kernel stays the same only for data with the same horizontal axis position\n",
    "    kernel = Gauss_kernel(RL_horizontal_axis_PSF[i])\n",
    "    with concurrent.futures.ProcessPoolExecutor(CPU_units_to_use) as pool:\n",
    "        intermediate_func = functools.partial(RL_iteration_for_concurrent, kernel)\n",
    "        res = pool.map(intermediate_func, RL_data_in_PSF[i])\n",
    "    RL_output_PSF[i] = np.array(list(res))\n",
    "    print(\"Finished iteration\", i+1, \"out of\", RL_data_in_PSF.shape[0], \"in\", np.round(time.time()-start_i, 2), \"seconds (\" + str(np.round(time.time()-start, 2)), \"from start).\")\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/PSF_RL_output.npy\", RL_output_PSF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e18254-c283-47c9-bce1-b5b3bd92b2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a chosen metric comparing the DAMN output with the corresponding target image\n",
    "RL_errors_PSF = Evaluate_metric(RL_output_PSF, RL_data_target_PSF)\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/PSF_RL_errors.npy\", RL_errors_PSF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424fc208-76b9-47c8-96b9-249bba3e1b83",
   "metadata": {},
   "source": [
    "## Evaluate dataset of the Emitter Concentration graph - panel (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cdf932-a07c-44b2-ba79-7453c3cd9369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data generated in the Data_generation notebook\n",
    "data_in_Conc = np.load(\"Data_files/Generated_data/Concentration_data_low_res.npy\")\n",
    "data_target_Conc = np.load(\"Data_files/Generated_data/Concentration_data_high_res.npy\")\n",
    "\n",
    "shape_Conc = data_in_Conc.shape\n",
    "print(\"Data shapes:\", shape_Conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1802d758-653f-466d-a050-b7e1466fdf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust the data shapes for easier model evaluation\n",
    "data_in_Conc_reshaped = np.reshape(data_in_Conc, (shape_Conc[0]*shape_Conc[1], shape_Conc[2], shape_Conc[3], 1))\n",
    "data_target_Conc_reshaped = np.reshape(data_target_Conc, (shape_Conc[0]*shape_Conc[1], shape_Conc[2], shape_Conc[3], 1))\n",
    "\n",
    "print(\"Data reshaped:\", data_in_Conc_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18d6824-efea-43c0-b23c-f6b24e45395c",
   "metadata": {},
   "source": [
    "### DAMN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f005a79a-e81b-49ae-840e-bb426f5f863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the DAMN model to predict high-resolution images\n",
    "DAMN_model_output_Conc_reshaped = np.squeeze(DAMN_model.predict(data_in_Conc_reshaped))\n",
    "\n",
    "#And return to the original shape\n",
    "DAMN_model_output_Conc = DAMN_model_output_Conc_reshaped.reshape((shape_Conc[0], shape_Conc[1], shape_Conc[2], shape_Conc[3]))\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/Concentration_DAMN_model_output.npy\", DAMN_model_output_Conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f065d7-061c-433c-a37f-6eeaa6ca09b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a chosen metric comparing the DAMN output with the corresponding target image\n",
    "DAMN_model_errors_Conc = Evaluate_metric(DAMN_model_output_Conc, data_target_Conc)\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/Concentration_DAMN_model_errors.npy\", DAMN_model_errors_Conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0071a6-f142-4e64-9b3c-d400c1804901",
   "metadata": {},
   "source": [
    "### Richardson-Lucy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200144e7-461b-4016-b647-666be853f642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the RL algorithm is very time consuming (matter of up to hours), you can choose to evaluate only a small portion of the data for the graph visualization\n",
    "#Namely, 1/10 of data samples in every 4-th point on the horizontal axis\n",
    "use_all_data_Conc = True             #Switch to False for evaluating only the reduced dataset \n",
    "\n",
    "if use_all_data_Conc:\n",
    "    RL_data_in_Conc = data_in_Conc\n",
    "    RL_data_target_Conc = data_target_Conc\n",
    "    RL_output_Conc = np.zeros(data_in_Conc.shape)\n",
    "else:\n",
    "    RL_data_in_Conc = data_in_Conc[::4,:10]\n",
    "    RL_data_target_Conc = data_target_Conc[::4,:10]\n",
    "    RL_output_Conc = np.zeros(data_in_Conc[::4,:10].shape)\n",
    "\n",
    "print(\"Using the following data shape for RL slgorithm:\", RL_data_in_Conc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da49b067-d727-4140-a0a8-3d061b65fb03",
   "metadata": {},
   "source": [
    "##### The following cell runs the Richardson-Lucy algorithm, which might turn very time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99a7dd-1ba6-46fa-b920-e779ab68a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The kernel stays the same for all Concentration data samples\n",
    "kernel = Gauss_kernel(PSF_width_value)\n",
    "\n",
    "#The ProcessPoolExecutor calling RL_iteration_for_concurrent function to evaluate the RL_data_in_Conc data\n",
    "start = time.time()\n",
    "for i in range(RL_data_in_Conc.shape[0]):\n",
    "    start_i = time.time()\n",
    "    with concurrent.futures.ProcessPoolExecutor(CPU_units_to_use) as pool:\n",
    "        intermediate_func = functools.partial(RL_iteration_for_concurrent, kernel)\n",
    "        res = pool.map(intermediate_func, RL_data_in_Conc[i])\n",
    "    RL_output_Conc[i] = np.array(list(res))\n",
    "    print(\"Finished iteration\", i+1, \"out of\", RL_data_in_Conc.shape[0], \"in\", np.round(time.time()-start_i, 2), \"seconds (\" + str(np.round(time.time()-start, 2)), \"from start).\")\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/Concentration_RL_output.npy\", RL_output_Conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3cd430-ff6f-4dcc-a55f-11e0464c1c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a chosen metric comparing the DAMN output with the corresponding target image\n",
    "RL_errors_Conc = Evaluate_metric(RL_output_Conc, RL_data_target_Conc)\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/Concentration_RL_errors.npy\", RL_errors_Conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b569fe41-b722-4a03-9cd2-6224ddeca467",
   "metadata": {},
   "source": [
    "## Evaluate dataset of the Airy-to-Gauss kernel transition graph - panel (D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce031d-ea5e-41df-8784-b57190532573",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data generated in the Data_generation notebook\n",
    "data_in_AtoG = np.load(\"Data_files/Generated_data/Transition_data_low_res.npy\")\n",
    "data_target_AtoG = np.load(\"Data_files/Generated_data/Transition_data_high_res.npy\")\n",
    "horizontal_axis_AtoG = np.load(\"Data_files/Generated_data/Transition_axis_array.npy\")\n",
    "\n",
    "shape_AtoG = data_in_AtoG.shape\n",
    "print(\"Data shapes:\", shape_AtoG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b66caf-6a85-42bc-83a4-57f877b6da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjust the data shapes for easier model evaluation\n",
    "data_in_AtoG_reshaped = np.reshape(data_in_AtoG, (shape_AtoG[0]*shape_AtoG[1], shape_AtoG[2], shape_AtoG[3], 1))\n",
    "data_target_AtoG_reshaped = np.reshape(data_target_AtoG, (shape_AtoG[0]*shape_AtoG[1], shape_AtoG[2], shape_AtoG[3], 1))\n",
    "\n",
    "print(\"Data reshaped:\", data_in_AtoG_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8464f525-744f-4381-aca4-5373940fdebf",
   "metadata": {},
   "source": [
    "### DAMN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0c483-92ed-4820-b1ab-2bcde0f0da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use the DAMN model to predict high-resolution images\n",
    "DAMN_model_output_AtoG_reshaped = np.squeeze(DAMN_model.predict(data_in_AtoG_reshaped))\n",
    "\n",
    "#And return to the original shape\n",
    "DAMN_model_output_AtoG = DAMN_model_output_AtoG_reshaped.reshape((shape_AtoG[0], shape_AtoG[1], shape_AtoG[2], shape_AtoG[3]))\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/Transition_DAMN_model_output.npy\", DAMN_model_output_AtoG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa2b305-4bcd-4706-b2ff-79dfe24ec3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a chosen metric comparing the DAMN output with the corresponding target image\n",
    "DAMN_model_errors_AtoG = Evaluate_metric(DAMN_model_output_AtoG, data_target_AtoG)\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/Transition_DAMN_model_errors.npy\", DAMN_model_errors_AtoG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193b485a-c011-47be-9afc-69b60962b933",
   "metadata": {},
   "source": [
    "### Richardson-Lucy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23124e9f-60b6-42c9-8292-24d963b81123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#As the RL algorithm is very time consuming (matter of up to hours), you can choose to evaluate only a small portion of the data for the graph visualization\n",
    "#Namely, 1/10 of data samples in every 4-th point on the horizontal axis\n",
    "use_all_data_AtoG = True             #Switch to False for evaluating only the reduced dataset \n",
    "\n",
    "if use_all_data_AtoG:\n",
    "    RL_data_in_AtoG = data_in_AtoG\n",
    "    RL_data_target_AtoG = data_target_AtoG\n",
    "    RL_output_AtoG = np.zeros(data_in_AtoG.shape)\n",
    "    RL_horizontal_axis_AtoG = horizontal_axis_AtoG\n",
    "else:\n",
    "    RL_data_in_AtoG = data_in_AtoG[::4,:10]\n",
    "    RL_data_target_AtoG = data_target_AtoG[::4,:10]\n",
    "    RL_output_AtoG = np.zeros(data_in_AtoG[::4,:10].shape)\n",
    "    RL_horizontal_axis_AtoG = horizontal_axis_AtoG[::4]\n",
    "\n",
    "print(\"Using the following data shape for RL slgorithm:\", RL_data_in_AtoG.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5be8f9-7c91-464d-abb5-a21407bba82f",
   "metadata": {},
   "source": [
    "##### The following cell runs the Richardson-Lucy algorithm, which might turn very time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8aad3b-5ef7-4393-add4-6432d4795735",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The ProcessPoolExecutor calling RL_iteration_for_concurrent function to evaluate the RL_data_in_AtoG data\n",
    "start = time.time()\n",
    "for i in range(RL_data_in_AtoG.shape[0]):\n",
    "    start_i = time.time()\n",
    "\n",
    "    #The kernel stays the same only for data with the same horizontal axis position\n",
    "    kernel = Get_AtoG_kernel(2, RL_horizontal_axis_AtoG[i])\n",
    "    with concurrent.futures.ProcessPoolExecutor(CPU_units_to_use) as pool:\n",
    "        intermediate_func = functools.partial(RL_iteration_for_concurrent, kernel)\n",
    "        res = pool.map(intermediate_func, RL_data_in_AtoG[i])\n",
    "    RL_output_AtoG[i] = np.array(list(res))\n",
    "    print(\"Finished iteration\", i+1, \"out of\", RL_data_in_AtoG.shape[0], \"in\", np.round(time.time()-start_i, 2), \"seconds (\" + str(np.round(time.time()-start, 2)), \"from start).\")\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/Transition_RL_output.npy\", RL_output_AtoG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b019e2-8189-49f3-bb0c-1b2c40c8529f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate a chosen metric comparing the DAMN output with the corresponding target image\n",
    "RL_errors_AtoG = Evaluate_metric(RL_output_AtoG, RL_data_target_AtoG)\n",
    "\n",
    "np.save(\"Data_files/Evaluated_data/Transition_RL_errors.npy\", RL_errors_AtoG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e956d04-831b-42cf-8502-d91df5ca2b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
